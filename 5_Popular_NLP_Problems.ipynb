{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOD8dZpR9MIR8y2zVlERved",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pratikagithub/DL-and-NLP-Projects/blob/main/5_Popular_NLP_Problems.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Problem 1: Process customer feedback scraped from a website, which contains HTML tags and special characters. Clean the text to prepare it for further analysis.***"
      ],
      "metadata": {
        "id": "bffizi4DkT3f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nQWYoZvrkKf1",
        "outputId": "2534888e-51ad-4d61-b962-a43780dff750"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Feedback: i love this product its amazing  visit us at\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# sample customer feedback\n",
        "feedback = \"<p>I <b>love</b> this product! It's amazing ðŸ˜Š. Visit us at https://example.com</p>\"\n",
        "\n",
        "# clean text\n",
        "def clean_text(text):\n",
        "    # remove HTML tags\n",
        "    text = BeautifulSoup(text, \"html.parser\").get_text()\n",
        "    # remove URLs\n",
        "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
        "    # remove special characters and emojis\n",
        "    text = re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text)\n",
        "    # convert to lowercase\n",
        "    text = text.lower().strip()\n",
        "    return text\n",
        "\n",
        "cleaned_feedback = clean_text(feedback)\n",
        "print(\"Cleaned Feedback:\", cleaned_feedback)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This solution uses a systematic approach to clean unstructured text data by removing noise like HTML tags, URLs, special characters, and emojis. It utilizes the BeautifulSoup library to strip HTML content and regular expressions (re) to identify and remove unwanted patterns such as URLs and non-alphanumeric characters. Finally, it converts the text to lowercase and trims whitespace, to ensure the processed text is clean and standardized for further analysis."
      ],
      "metadata": {
        "id": "KF3HjRPKkjgO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Problem 2: Given a set of customer reviews, extract the most common bigrams (two-word combinations) to identify popular themes.***"
      ],
      "metadata": {
        "id": "gQ2Jz6vYkoEJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# sample reviews\n",
        "reviews = [\n",
        "    \"The delivery was fast and smooth.\",\n",
        "    \"Customer service was polite and helpful.\",\n",
        "    \"The product quality exceeded expectations.\",\n",
        "    \"Delivery was delayed but resolved quickly.\"\n",
        "]\n",
        "\n",
        "# extract bigrams\n",
        "vectorizer = CountVectorizer(ngram_range=(2, 2), stop_words=\"english\")\n",
        "bigram_matrix = vectorizer.fit_transform(reviews)\n",
        "\n",
        "# get most common bigrams\n",
        "bigram_counts = bigram_matrix.toarray().sum(axis=0)\n",
        "bigram_features = vectorizer.get_feature_names_out()\n",
        "\n",
        "# sort and display\n",
        "bigram_dict = dict(zip(bigram_features, bigram_counts))\n",
        "sorted_bigrams = sorted(bigram_dict.items(), key=lambda x: x[1], reverse=True)\n",
        "print(\"Most Common Bigrams:\", sorted_bigrams[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jUHnvYhkwEC",
        "outputId": "9c0ef855-04cf-4493-97a3-8db8960bccd6"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most Common Bigrams: [('customer service', 1), ('delayed resolved', 1), ('delivery delayed', 1), ('delivery fast', 1), ('exceeded expectations', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This solution identifies the most common bigrams (two-word combinations) in a text dataset by leveraging the CountVectorizer from scikit-learn. It uses ngram_range=(2, 2) to extract bigrams while removing stopwords for cleaner results. The process sums, sorts, and displays the resulting bigram frequencies to provide insights into popular word pairings in the text. This approach is effective for understanding themes or patterns in textual datasets."
      ],
      "metadata": {
        "id": "ZJtTDcIBk3LD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Problem 3: You are given a multilingual dataset of tweets. Detect and separate tweets written in English for analysis.***"
      ],
      "metadata": {
        "id": "WWYft_yDk7a7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langdetect\n",
        "from langdetect import detect\n",
        "\n",
        "# sample tweets\n",
        "tweets = [\n",
        "    \"I love natural language processing!\",\n",
        "    \"Me encanta el procesamiento del lenguaje natural.\",\n",
        "    \"J'adore le traitement du langage naturel.\"\n",
        "]\n",
        "\n",
        "# detect and filter English tweets\n",
        "tweets = print(\"English Tweets:\", tweets)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n7gRlwMnk_5j",
        "outputId": "44814e73-93b4-4511-8c90-afc80760fdde"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langdetect in /usr/local/lib/python3.10/dist-packages (1.0.9)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "English Tweets: ['I love natural language processing!', 'Me encanta el procesamiento del lenguaje natural.', \"J'adore le traitement du langage naturel.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This solution detects the language of text data using the langdetect library and filters it based on a specified criterion (e.g., English tweets). For each tweet in the dataset, the detect function identifies its language. The process selects tweets classified as English (language code â€œenâ€) and stores them in a separate list. This approach is practical for preprocessing multilingual datasets and isolating language-specific data for further analysis."
      ],
      "metadata": {
        "id": "_YCb5x_MlDQq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Problem 4: Identify and remove duplicate or near-duplicate customer queries in a support ticket dataset.***"
      ],
      "metadata": {
        "id": "YYmlyeJ6ln7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# sample support tickets\n",
        "tickets = [\n",
        "    \"How can I reset my password?\",\n",
        "    \"How do I change my password?\",\n",
        "    \"What is the process to reset my password?\",\n",
        "    \"Can I update my profile details?\"\n",
        "]\n",
        "\n",
        "# vectorize tickets\n",
        "vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
        "tfidf_matrix = vectorizer.fit_transform(tickets)\n",
        "\n",
        "# compute cosine similarity\n",
        "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
        "\n",
        "# identify duplicates (threshold > 0.5 similarity)\n",
        "duplicates = []\n",
        "for i in range(len(tickets)):\n",
        "    for j in range(i + 1, len(tickets)):\n",
        "        if similarity_matrix[i, j] > 0.5:\n",
        "            duplicates.append((tickets[i], tickets[j]))\n",
        "\n",
        "print(\"Duplicate Tickets:\", duplicates)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XvcI_97ylnor",
        "outputId": "91b68015-38c2-41d8-db80-95609734f2e3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Duplicate Tickets: [('How can I reset my password?', 'What is the process to reset my password?')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This solution detects duplicate or near-duplicate text entries using cosine similarity on TF-IDF vectorized text data. TfidfVectorizer converts each ticket into a numerical feature matrix. The matrix captures term importance and ignores common stopwords. The process calculates the cosine similarity matrix for pairwise ticket comparisons. Entries with a similarity score above 0.5 are flagged as duplicates. This method effectively identifies highly similar text entries for deduplication or clustering tasks."
      ],
      "metadata": {
        "id": "d9Xwt9dTlv31"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Problem 5: Analyze the sentiment of customer reviews over the past month to identify weekly trends.***"
      ],
      "metadata": {
        "id": "HsCtRrWClx5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from textblob import TextBlob\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# sample data\n",
        "data = {\n",
        "    \"review\": [\n",
        "        \"The service was excellent.\",\n",
        "        \"Terrible experience, very dissatisfied.\",\n",
        "        \"Decent product, met expectations.\",\n",
        "        \"Absolutely loved it, will buy again!\"\n",
        "    ],\n",
        "    \"date\": [\"2024-12-01\", \"2024-12-02\", \"2024-12-08\", \"2024-12-15\"]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# compute sentiment\n",
        "df[\"sentiment\"] = df[\"review\"].apply(lambda x: TextBlob(x).polarity)\n",
        "df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
        "\n",
        "# weekly sentiment trend\n",
        "df.set_index(\"date\", inplace=True)\n",
        "weekly_sentiment = df[\"sentiment\"].resample(\"W\").mean()\n",
        "print(\"Weekly Sentiment Trend:\")\n",
        "print(weekly_sentiment)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kxZgb4Xvl1Wx",
        "outputId": "be867bde-54f9-4022-e0fc-140f776cbee1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Weekly Sentiment Trend:\n",
            "date\n",
            "2024-12-01    1.000000\n",
            "2024-12-08   -0.116667\n",
            "2024-12-15    0.875000\n",
            "Freq: W-SUN, Name: sentiment, dtype: float64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This solution analyzes the sentiment trends over time by first computing the sentiment polarity of each review using TextBlob, where polarity ranges from -1 (negative) to 1 (positive). The review dates are converted into datetime objects using pandas for proper time-based analysis. This approach prepares the data for aggregating sentiment trends."
      ],
      "metadata": {
        "id": "kD2HGn1Kl4ZO"
      }
    }
  ]
}